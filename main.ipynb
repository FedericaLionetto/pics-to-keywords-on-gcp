{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c317a72-5f3c-435a-a6a5-ce7860f27bb2",
   "metadata": {},
   "source": [
    "# Keep track of your pictures: how to automatically add keywords + how to build and maintain an image database\n",
    "\n",
    "Author: Federica Lionetto  \n",
    "Email: federica.lionetto@gmail.com  \n",
    "Date: 30 April 2022  \n",
    "\n",
    "This work is licensed under a <a rel=\"license\" href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
    "\n",
    "## Objective\n",
    "\n",
    "Let's consider the following scenario.\n",
    "You love taking pictures and always have your camera with you to be ready to capture the best moments of life.\n",
    "However, most of your pictures simply go from your memory card to your external hard drive and you would have no idea where to find what.\n",
    "You considered several times the possibility to add keywords to your pictures to be able to easily find them later on.\n",
    "Maybe you even started with that, but gave up after a few attempts.\n",
    "Does this sound familiar to you?\n",
    "\n",
    "Generating keywords is a time-consuming task, and is likely to be very boring as well.\n",
    "Instead of generating keywords manually, in this short tutorial we will draw on the power of the cloud and let the `Vision AI` API on Google Cloud solve this repetitive task for us.\n",
    "We will also go through a way to keep our pictures organized into a database on Google Cloud.\n",
    "\n",
    "For each image in our collection, we will store the following information in the database:\n",
    "- `file_name`, that is, the name of the image\n",
    "- `creation_date_time`, that is, the date and time of the creation of the image\n",
    "- `keywords`, that is, the keywords associated with the image \n",
    "\n",
    "It's time to get started!\n",
    "\n",
    "## Cloud settings\n",
    "\n",
    "To run the tutorial, we need the following resources on Google Cloud: \n",
    "- a `Cloud Storage` bucket, where we will upload the pictures\n",
    "- a `BigQuery` dataset, where we will store the information about the pictures \n",
    "- a `Vertex AI` notebook, where we will write and run our Python code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb891460-7510-4873-8ae4-bd5f34053ab4",
   "metadata": {},
   "source": [
    "You can create the `Cloud Storage` bucket, the `BigQuery` dataset, and the `Vertex AI` notebook from the Console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517eff47-37bb-4b67-ad93-63411421618b",
   "metadata": {},
   "source": [
    "## Step 1: Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8552042-3021-4a7b-8f7c-67941320f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "from google.cloud import vision\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# %load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e564deb-0507-4fe3-9ceb-ae3aeb217146",
   "metadata": {},
   "source": [
    "## Step 2: Configuration\n",
    "\n",
    "Here we can configure some of the variables that are used throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b121f-4486-40a7-83dc-5cfd57109a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "region = \"[REGION GOES HERE]\" # Where to create the resources on Google Cloud\n",
    "gcp_project_name = \"[GCP PROJECT NAME GOES HERE]\" # The name of the GCP project\n",
    "\n",
    "bucket_name = \"[BUCKET NAME GOES HERE]\" # The name of the Cloud Storage bucket\n",
    "folder_name_landing = \"image/landing\" # The name of the folder in the Cloud Storage bucket where new pictures are uploaded\n",
    "folder_name_archive = \"image/archive\" # The name of the folder in the Cloud Storage bucket where processed pictures are archived\n",
    "\n",
    "scale = 0.2 # Used to scale images for easier visualization\n",
    "\n",
    "bq_dataset_name = '[DATASET NAME GOES HERE]' # The name of the BigQuery dataset\n",
    "bq_table_name = 'image' # The name of the BigQuery table\n",
    "\n",
    "sample_image_name = '[SAMPLE IMAGE NAME GOES HERE]' # A sample image to use as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc8765-c41a-4e1d-b4d8-9afe4c601d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "region = \"europe-west6\" # Where to create the resources on Google Cloud\n",
    "gcp_project_name = \"personalproject-348318\" # The name of the GCP project\n",
    "\n",
    "bucket_name = \"whitebloomingtulip-input-164\" # The name of the Cloud Storage bucket\n",
    "folder_name_landing = \"image/landing\" # The name of the folder in the Cloud Storage bucket where new pictures are uploaded\n",
    "folder_name_archive = \"image/archive\" # The name of the folder in the Cloud Storage bucket where processed pictures are archived\n",
    "\n",
    "scale = 0.2 # Scale images for easier visualization\n",
    "\n",
    "bq_dataset_name = 'whitebloomingtulip_db' # The name of the BigQuery dataset\n",
    "bq_table_name = 'image' # The name of the BigQuery table\n",
    "\n",
    "sample_image_name = 'IMG_3579.jpeg' # A sample image to use as an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cfaf10-cd2b-4c89-8d72-c79f72f0113d",
   "metadata": {},
   "source": [
    "## 3: Instantiate the clients\n",
    "\n",
    "We need to instantiate three clients: one for `Cloud Storage`, one for `BigQuery`, and one for the `Vision AI API`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300bb5cd-cbee-4cef-aa2e-20471c505b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the clients\n",
    "storage_client = storage.Client()\n",
    "vision_client = vision.ImageAnnotatorClient()\n",
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c4b43-d56d-4783-b991-983e3a04387f",
   "metadata": {},
   "source": [
    "## 4: Copy images from GCS to the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b628783f-0bee-4cfb-917c-e3be42f30792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 folders (if they do not exist), one for images and one for keywords\n",
    "if not os.path.exists('images'):\n",
    "    os.mkdir('images')\n",
    "if not os.path.exists('keywords'):\n",
    "    os.mkdir('keywords')\n",
    "\n",
    "# Create an empty list for the images to be copied\n",
    "file_names = []\n",
    "\n",
    "# Access the GCS bucket containing the images to be copied\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "# If the images are in a subfolder of the GCS bucket, specify the subfolder structure\n",
    "prefix_landing = f\"{folder_name_landing}/\" \n",
    "blobs_landing = bucket.list_blobs(prefix = prefix_landing, delimiter = '/')\n",
    "\n",
    "for blob in blobs_landing:\n",
    "    if(blob.name != prefix_landing): # Ignore the subfolder itself \n",
    "        file_name = blob.name.replace(prefix_landing, \"\")\n",
    "        blob.download_to_filename('images/'+file_name) # Download the file to the machine\n",
    "        file_names.append('images/'+file_name)\n",
    "\n",
    "print(\"Images:\")\n",
    "print(file_names)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90638d52-8867-4d96-a462-2638d51c61f7",
   "metadata": {},
   "source": [
    "## 5: Generate keywords and display their score\n",
    "\n",
    "The `Vision AI` API allows to annotate an image with keywords that describe the contents of that image.\n",
    "Each keyword has an associated score, where a higher score means that the algorithm has a higher confidence that the keyword describes something in the image.\n",
    "\n",
    "You can try out the `Vision AI` API interactively here: https://cloud.google.com/vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec59527-5a55-40b3-89ad-5a9a09cbe42d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_annotation_desc_dict = {} \n",
    "label_annotation_score_dict = {} \n",
    "# file_names_out = []\n",
    "\n",
    "for file_name in file_names:\n",
    "    # Get the two parts of the file name\n",
    "    file_name_without_extension = file_name.rsplit('.', 1)[0]\n",
    "    file_name_extension = file_name.rsplit('.', 1)[1]\n",
    "    if verbose:\n",
    "        print('File name without extension:', file_name_without_extension)\n",
    "        print('File name extension:', file_name_extension)\n",
    "        print('')\n",
    "    \n",
    "    # Display selected image\n",
    "    display(Image(filename=file_name, width=500))\n",
    "    \n",
    "    # Annotate selected image\n",
    "    file_name = os.path.abspath(file_name)\n",
    "\n",
    "    # Load the image into memory\n",
    "    with io.open(file_name, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.Image(content=content)\n",
    "\n",
    "    # Perform label detection on the image\n",
    "    response = vision_client.label_detection(image=image)\n",
    "    if verbose:\n",
    "        print('Response:')\n",
    "        print(response)\n",
    "        print('')\n",
    "        print('Label annotations:')\n",
    "        print(response.label_annotations)\n",
    "        print('')\n",
    "        print('First element of label annotations:')\n",
    "        print(response.label_annotations[0])\n",
    "        print('')\n",
    "        print('Description of first element of label annotations:')\n",
    "        print(response.label_annotations[0].description)\n",
    "        print('')\n",
    "    \n",
    "    labels = response.label_annotations\n",
    "\n",
    "    print('Keywords:')\n",
    "    for label in labels:\n",
    "        print(label.description)\n",
    "    print('')\n",
    "        \n",
    "    # Create lists of description and score for selected image\n",
    "    n_label_annotations = len(response.label_annotations)\n",
    "\n",
    "    label_annotation_desc = []\n",
    "    label_annotation_score = []\n",
    "\n",
    "    for i in range(n_label_annotations):\n",
    "        label_annotation_desc.append(response.label_annotations[i].description)\n",
    "        label_annotation_score.append(response.label_annotations[i].score)\n",
    "    print('List of keywords:')\n",
    "    print(label_annotation_desc)\n",
    "    print('')\n",
    "    print('List of scores:')\n",
    "    print(label_annotation_score)\n",
    "    print('')\n",
    "\n",
    "    label_annotation_desc_dict[file_name] = label_annotation_desc\n",
    "    label_annotation_score_dict[file_name] = label_annotation_score\n",
    "    \n",
    "    # Display label annotations (description and score) for selected image\n",
    "    plt.figure()\n",
    "    sns.barplot(x=label_annotation_score, y=label_annotation_desc, color='red')\n",
    "    plt.savefig(file_name_without_extension.replace('images/', 'keywords/')+'_keywords', format='png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create filename based on label annotations\n",
    "    # file_name_out = \"_\".join(label_annotation_desc)\n",
    "    # file_name_out = file_name_out.replace(\" \", \"-\")\n",
    "    # file_name_out = file_name_out+\".jpeg\"\n",
    "    # print(\"Input file name:\", file_name)\n",
    "    # print('')\n",
    "    # print(\"Output file name:\", file_name_out)\n",
    "    # print('')\n",
    "    # file_names_out.append(file_name_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6987a21-514e-4493-b6e2-641cec40731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dictionary of list of keywords:')\n",
    "print(label_annotation_desc_dict)\n",
    "print('')\n",
    "print('Dictionary of list of scores:')\n",
    "print(label_annotation_score_dict)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74715b36-01ae-42fc-b108-d4cf56782668",
   "metadata": {},
   "source": [
    "## 6: Create database with keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8fc22-a70a-4581-8c97-3cddfe956921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bq_dataset = bq_client.dataset(bq_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cdbb1-fbc9-4314-a476-4fc23c8115a4",
   "metadata": {},
   "source": [
    "### Create empty table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0db533-61f1-4331-9604-d34540967431",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_create = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS `{gcp_project_name}.{bq_dataset_name}.{bq_table_name}`\n",
    "(\n",
    "file_name STRING OPTIONS(description=\"The name of the file\"),\n",
    "creation_date_time DATETIME OPTIONS(description=\"The date and time when the file was uploaded to the cloud\"),\n",
    "keywords ARRAY<STRING> OPTIONS(description=\"The keywords associated to the image\")\n",
    ")\n",
    "PARTITION BY DATETIME_TRUNC(creation_date_time, DAY)\n",
    "OPTIONS(\n",
    "description=\"Image database\"\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print('Query for creating empty table:')\n",
    "print(query_create)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8289c5bc-b165-443d-90f7-4d342e2b4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query\n",
    "query_job_create = bq_client.query(query_create, location=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c60642-6abf-40f5-b45d-86c29fc8d1aa",
   "metadata": {},
   "source": [
    "### Insert one row in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d2981-a463-462c-a3a2-a8252436148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_date_time = datetime.datetime.fromtimestamp(os.path.getmtime('/home/jupyter/images/'+sample_image_name))\n",
    "print('Creation date time:')\n",
    "print(creation_date_time)\n",
    "print('')\n",
    "\n",
    "# Insert one row\n",
    "query_insert_one = f\"\"\"\n",
    "INSERT `{gcp_project_name}.{bq_dataset_name}.{bq_table_name}` (file_name, creation_date_time, keywords) \n",
    "VALUES('/home/jupyter/images/{sample_image_name}', '{creation_date_time}', {label_annotation_desc_dict[f'/home/jupyter/images/{sample_image_name}']})\n",
    "\"\"\"\n",
    "\n",
    "print('Query for inserting one row:')\n",
    "print(query_insert_one)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312f13b-6580-40e2-86ae-75e6eac00b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query\n",
    "query_job_insert_one = bq_client.query(query_insert_one, location=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c384af1-b6c8-4663-8ee5-ec7da5fda799",
   "metadata": {},
   "source": [
    "### Insert multiple rows in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe5c24-425c-43cb-90e7-3442501d4460",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "values_str = ''\n",
    "for key in label_annotation_desc_dict.keys():\n",
    "    creation_date_time_temp = datetime.datetime.fromtimestamp(os.path.getmtime(key))\n",
    "    keywords_temp = label_annotation_desc_dict[key]\n",
    "    if values_str == '':\n",
    "        values_str = values_str+f\"VALUES('{key}', '{creation_date_time_temp}', {keywords_temp})\"\n",
    "    else:\n",
    "        values_str = values_str+f\",('{key}', '{creation_date_time_temp}', {keywords_temp})\"\n",
    "    if verbose:\n",
    "        print(values_str)\n",
    "        print('')\n",
    "\n",
    "print('String for inserting multiple rows:')\n",
    "print(values_str)    \n",
    "print('')\n",
    "\n",
    "# Insert multiple rows\n",
    "query_insert_many = f\"\"\"\n",
    "INSERT `{gcp_project_name}.{bq_dataset_name}.{bq_table_name}` (file_name, creation_date_time, keywords) \n",
    "{values_str}\n",
    "\"\"\"\n",
    "\n",
    "print('Query for inserting multiple rows:')\n",
    "print(query_insert_many)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64abc363-ab6f-4f4d-a974-60a1c708e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query\n",
    "query_job_insert_many = bq_client.query(query_insert_many, location=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2d45e-78e4-476b-b615-e2dfd6ede777",
   "metadata": {},
   "source": [
    "### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf4eb3-26c1-4b53-8af5-082919b5beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated rows\n",
    "query_drop_duplicates = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{gcp_project_name}.{bq_dataset_name}.{bq_table_name}` \n",
    "PARTITION BY DATETIME_TRUNC(creation_date_time, DAY) AS (\n",
    "  SELECT \n",
    "    * EXCEPT(row_number) \n",
    "  FROM (\n",
    "    SELECT\n",
    "      *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY file_name) row_number\n",
    "    FROM \n",
    "        `{gcp_project_name}.{bq_dataset_name}.{bq_table_name}`)\n",
    "WHERE row_number = 1\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print('Query for dropping duplicates:')\n",
    "print(query_drop_duplicates)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c59a6d7-9fec-487b-965a-28544960dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query\n",
    "query_job_drop_duplicates = bq_client.query(query_drop_duplicates, location=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6936c626-04ed-4880-83e8-7f5a680cb3f8",
   "metadata": {},
   "source": [
    "### Read the contents of the table\n",
    "\n",
    "This is used to cross-check the contents of the table by importing the data in a `Pandas` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537cdd1a-683e-42f9-97cf-4ff0921d19dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve rows from table\n",
    "query_read = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{gcp_project_name}.{bq_dataset_name}.{bq_table_name}`\n",
    "\"\"\"\n",
    "\n",
    "print('Query for reading the contents of the table:')\n",
    "print(query_read)\n",
    "print('')\n",
    "\n",
    "# Execute the query\n",
    "query_job_read = bq_client.query(query_read, location=region)\n",
    "\n",
    "df = query_job_read.result().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d4e05-6c93-4a52-a6c9-b782c4be0f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10cba2-8a14-4050-8fb2-c5e57eb422d0",
   "metadata": {},
   "source": [
    "## 7: Archive processed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88cc01-c7ee-4285-a1c8-90ac73c20009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the images must be archived in a subfolder of the GCS bucket, specify the subfolder structure\n",
    "prefix_archive = f\"{folder_name_archive}/\" \n",
    "blobs_landing = bucket.list_blobs(prefix = prefix_landing, delimiter = '/')\n",
    "\n",
    "for blob in blobs_landing:\n",
    "    if(blob.name != prefix_landing): # Ignore the subfolder itself \n",
    "        bucket.rename_blob(blob, new_name=blob.name.replace(prefix_landing, prefix_archive))\n",
    "        print(f'{blob.name} renamed to {blob.name.replace(prefix_landing, prefix_archive)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea158e-9225-4e6f-b224-b55ce1cb0df7",
   "metadata": {},
   "source": [
    "## 8: Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f3447e-0def-420e-b81d-fc48cf685783",
   "metadata": {},
   "source": [
    "### Clean up the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c558e1-2471-4f82-8cb6-e3663dd3debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all files in the machine\n",
    "shutil.rmtree('images')\n",
    "shutil.rmtree('keywords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de7a5fa-49e5-483a-b15a-3baed3612c91",
   "metadata": {},
   "source": [
    "### Clean up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1037968-ba2b-4250-82cc-c00ee6bc858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the images back from archive to landing\n",
    "blobs_archive = bucket.list_blobs(prefix = prefix_archive, delimiter = '/')\n",
    "\n",
    "for blob in blobs_archive:\n",
    "    if(blob.name != prefix_archive): # Ignore the subfolder itself \n",
    "        bucket.rename_blob(blob, new_name=blob.name.replace(prefix_archive, prefix_landing))\n",
    "        print(f'{blob.name} renamed to {blob.name.replace(prefix_archive, prefix_landing)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a77ffc-c991-4e9b-92d2-4e132e026878",
   "metadata": {},
   "source": [
    "### Clean up BigQuery\n",
    "\n",
    "Run this only if you want to delete the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b604a9ac-b93f-4c14-81ea-c9d2d5ebba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows in the table\n",
    "query_del = f\"\"\"\n",
    "DROP TABLE `{gcp_project_name}.{bq_dataset_name}.{bq_table_name}`\n",
    "\"\"\"\n",
    "\n",
    "print('Query for deleting all rows in the table:')\n",
    "print(query_del)\n",
    "print('')\n",
    "\n",
    "# Execute the query\n",
    "query_job_del = bq_client.query(query_del, location=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ae20e-7541-40c1-a1f5-d1dcecb6543f",
   "metadata": {},
   "source": [
    "## Possible extensions\n",
    "\n",
    "So far we assumed that, after uploading new pictures to Google Cloud, we manually execute the code in the `Vertex AI` notebook to generate the keywords and update the database.\n",
    "One step further could be to trigger the execution of the code when one or more files are uploaded to the relevant `Cloud Storage` bucket in Google Cloud. We can do that in a couple of different ways. \n",
    "One way is to use a `Cloud Function`. `Cloud Functions` are a function-as-a-service (FaaS) product that allows you to execute your code without having to worry about infrastructure (no servers and no containers to manage) and to pay only for the execution time of the code. `Cloud Functions` are event-driven and can be triggered by several events related to `Cloud Storage`, in particular:\n",
    "- object creation\n",
    "- object deletion\n",
    "- object archiving\n",
    "- metadata updates\n",
    "\n",
    "Another way to achieve the same result is to use `Dataflow`, a product that allows to deal with batch and stream data processing in a serverless way. \n",
    "`Dataflow` is built around `Apache Beam`, an open-source model for defining batch and stream data processing pipelines.\n",
    "\n",
    "Instead of having our code triggered by an event, we could decide to execute it according to a certain schedule, for example once a day or once a week.\n",
    "This can be done using `Composer`, a workflow orchestration service built on `Apache Airflow`. \n",
    "\n",
    "## References\n",
    "\n",
    "- `Cloud Storage` on Google Cloud: https://cloud.google.com/storage\n",
    "- `BigQuery` on Google Cloud: https://cloud.google.com/bigquery\n",
    "- `Vision AI` API on Google Cloud: https://cloud.google.com/vision\n",
    "- `Cloud Functions` on Google Cloud: https://cloud.google.com/functions\n",
    "- `Dataflow` on Google Cloud: https://cloud.google.com/dataflow\n",
    "- `Composer` on Google Cloud: https://cloud.google.com/composer\n",
    "- How to use the `BigQuery` API: https://cloud.google.com/bigquery/docs/quickstarts/quickstart-client-libraries\n",
    "- How to specify a schema in `BigQuery`: https://cloud.google.com/bigquery/docs/schemas\n",
    "- How to create a partitioned table in `BigQuery`: https://cloud.google.com/bigquery/docs/creating-partitioned-tables\n",
    "- How to write the results of a query in `BigQuery`: https://cloud.google.com/bigquery/docs/writing-results\n",
    "- How to download `BigQuery` data to a `Pandas` dataframe: https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas\n",
    "- How to trigger a `Cloud Function` from a `Cloud Storage` event: https://cloud.google.com/functions/docs/calling/storage\n",
    "- `Apache Beam` programming guide, including triggers: https://beam.apache.org/documentation/programming-guide/ \n",
    "- `Apache Airflow`: https://airflow.apache.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185e743e-70cf-41a5-aa9e-1af8040fbbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
